{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepaktamil20/Mini_Projects/blob/main/Heart_Disease_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_drqRn3ebnZ"
      },
      "source": [
        "<table align=\"left\" width=100%>\n",
        "    <tr>\n",
        "        <td width=\"10%\">\n",
        "        </td>\n",
        "        <td>\n",
        "            <div align=\"left\">\n",
        "                <font color=\"#21618C\" size=8px>\n",
        "                  <b>Heart Disease Prediction\n",
        "                    </b>\n",
        "                </font>\n",
        "            </div>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OXjYS1webna"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "Heart disease is easier to treat when it is detected in the early stages. Machine learning techniques may aid a more efficient analysis in the prediction of the disease. Moreover, this prediction is one of the most central problems in medical, as it is one of the leading disease related to unhealthy lifestyle. So, an early prediction of this disease will be useful for a cure or averion. In this study, we experiment with the heart disease dataset to explore the machine learning algorithms and build an optimum model to predict the disease.                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQO22FFqebnb"
      },
      "source": [
        "## Data Definition\n",
        "\n",
        "Each attribute is a medical risk factor.\n",
        "\n",
        "    \n",
        "**age**: Age of the patient - (Numerical)\n",
        "\n",
        "**gender**: Gender of the patient - (0,1) - (Male, Female) - (Categorical) \n",
        " \n",
        "**chest_pain**: It refers to the chest pain experienced by the patient -(0,1,2,3) - (Categorical)\n",
        "    \n",
        "**rest_bps**: Blood pressure of the patient while resting(in mm/Hg) - (Numerical)\n",
        "    \n",
        "**cholestrol**: Patient's cholestrol level (in mg/dl) - (Numerical)\n",
        "    \n",
        "**fasting_blood_sugar**: Blood sugar of the patient while fasting - (>120mg/: = 1, otherwise = 0) - (Categorical)\n",
        "    \n",
        "**rest_ecg**: Potassium level (0,1,2) - (Categorical)\n",
        "    \n",
        "**thalach**: The patients maximum heart rate - (Numerical)\n",
        "    \n",
        "**exer_angina**: It refers to the exercise induced angina - (1=Yes, 0=No) - (Categorical)\n",
        "    \n",
        "**old_peak**: It is the ST depression induced by exercise relative to rest(ST relates to the position on ECG plots)  (Numerical)\n",
        "    \n",
        "**slope**:  It refers to the slope of the peak of the exercise ST Segment- (0,1,2) - (Categorical)\n",
        "    \n",
        "**ca**: Number of major vessels - (0,1,2,3,4) - (Categorical)\n",
        "    \n",
        "**thalassemia**: It refers to thalassemia which is a blood disorder - (0,1,2,3) - (Categorical)\n",
        " \n",
        "**target**: Patient has heart disease or not - (1=Yes, 0=No) - (Target variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyalDMN-ebnc"
      },
      "source": [
        "<a id='import_lib'></a>\n",
        "# 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lBgh1J0Mebnd"
      },
      "outputs": [],
      "source": [
        "# suppress display of warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "\n",
        "# 'Pandas' is used for data manipulation and analysis\n",
        "import pandas as pd \n",
        "\n",
        "# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\n",
        "import seaborn as sns\n",
        "\n",
        "# import 'is_string_dtype' to check if the type of input is string  \n",
        "from pandas.api.types import is_string_dtype\n",
        "\n",
        "# import various functions to perform classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import tree\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# import functions to perform logistic regression\n",
        "import statsmodels\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# import functions to plot the decision tree\n",
        "import pydotplus\n",
        "from IPython.display import Image  \n",
        "import graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS_fvg4Uwwwq"
      },
      "outputs": [],
      "source": [
        "! pip install pydotplus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIn4c1GJwwws"
      },
      "outputs": [],
      "source": [
        "! pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8jhFmilpwwwt"
      },
      "outputs": [],
      "source": [
        "# set the plot size using 'rcParams'\n",
        "# once the plot size is set using 'rcParams', it sets the size of all the forthcoming plots in the file\n",
        "# pass width and height in inches to 'figure.figsize' \n",
        "plt.rcParams['figure.figsize'] = [15,8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll0ntaGOebng"
      },
      "source": [
        "<a id='set_options'></a>\n",
        "# 2. Set Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qJcw0J2Cebnh"
      },
      "outputs": [],
      "source": [
        "# display all columns of the dataframe\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# display all rows of the dataframe\n",
        "pd.options.display.max_rows = None\n",
        "\n",
        "# use below code to convert the 'exponential' values to float\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9nOxSTeawwwu"
      },
      "outputs": [],
      "source": [
        "#os.chdir('/Users/suchita/DEsktop/PythonSession/HeartDiseaseCaseStudy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPb9_if1ebnl"
      },
      "source": [
        "<a id='RD'></a>\n",
        "# 3. Read Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ua8KGqLSw_y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDTwhMqAebnm"
      },
      "outputs": [],
      "source": [
        "# read the excel data file \n",
        "df_Heart = pd.read_csv(\"/content/drive/MyDrive/0.MKCE/4.Decision Tree/2 Project/Heart Disease Prediction/Dataset/HeartDisease.csv\")\n",
        "\n",
        "# display the top 5 rows of the dataframe\n",
        "df_Heart.head()\n",
        "\n",
        "# Note: To display more rows, example 10, use head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf8x7xFiwwww"
      },
      "source": [
        "#### Dimensions of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6QIB-Nowwww"
      },
      "outputs": [],
      "source": [
        "# 'shape' function gives the total number of rows and columns in the data\n",
        "df_Heart.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSHIYKY2ebns"
      },
      "source": [
        "<a id='data_preparation'></a>\n",
        "# 4. Data Analysis and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-baDwGWebnu"
      },
      "source": [
        "<a id='Data_Understanding'></a>\n",
        "## 4.1 Understand the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O71NsesEebn3"
      },
      "source": [
        "<a id='Data_Types'></a>\n",
        "### 4.1.1 Data Type\n",
        "The main data types in Pandas dataframes are the object, float, int64, bool, and datetime64. To understand each attribute of our data, it is always good for us to know the data type of each column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68w94ahlwwwy"
      },
      "source": [
        "**1. Check for the data type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnYrzq7Yebn4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# 'dtypes' gives the data type for each column\n",
        "df_Heart.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTAsWvPwwwz"
      },
      "source": [
        "**2. Change the incorrect data type.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ogPgThuFebn8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# use 'for' loop to change the data type of variables \n",
        "for col in ['gender','chest_pain','fasting_blood_sugar','rest_ecg','exer_angina','slope','ca','thalassemia']:\n",
        "     \n",
        "    # use .astype() to change the data type\n",
        "    df_Heart[col] = df_Heart[col].astype('object')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rG9ZJ7bebn-"
      },
      "source": [
        "**3. Recheck the data type after the conversion.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp6_mQOBebn_"
      },
      "outputs": [],
      "source": [
        "# recheck the data types of all variables\n",
        "df_Heart.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ILGZCscuwww1"
      },
      "outputs": [],
      "source": [
        "# splitting features and the target variable\n",
        "# consider all the columns except 'target' using 'iloc'\n",
        "df_features = df_Heart.iloc[:, df_Heart.columns != 'target']\n",
        "\n",
        "# consider the target variable\n",
        "df_target = df_Heart.iloc[:,df_Heart.columns == 'target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCHHAKbIwww1"
      },
      "source": [
        "Use the dataframe containing features (df_features) for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFOkFedXeboC"
      },
      "source": [
        "<a id='Summary_Statistics'></a>\n",
        "### 4.1.2 Summary Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSPelw83www2"
      },
      "source": [
        "**1. For numerical variables, use the describe()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCq-Il09eboC",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# the describe() returns the statistical summary of the variables\n",
        "# by default, it returns the summary of numerical variables\n",
        "#We transpose the results for better readability\n",
        "df_features.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUylzUstwww2"
      },
      "source": [
        "**2. For categorical variables, use the describe(include=object)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7xasuY6eboF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# summary of the categorical variables \n",
        "#df_features.describe(include = object)\n",
        "#Transpose the result for better readability\n",
        "df_features.describe(include='object').transpose()\n",
        "# Note: if we pass 'include=object' to the .describe() function returns descriptive statistics for categorical variables only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OehAU0ZKwww3"
      },
      "source": [
        "<a id='distribution_variables'></a>\n",
        "### 4.1.3 Distribution of Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tWX9ptGeboe"
      },
      "source": [
        "#### 1. Distribution of numeric independent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai5_xbHWwww4"
      },
      "source": [
        "For the independent numeric variables, we plot the histogram to check the distribution of the variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne5rQHn5ebof",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# plot the histogram of numeric independent variables\n",
        "# the hist() function considers the numeric variables only, by default\n",
        "df_features.hist()\n",
        "\n",
        "# adjust the subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1hJh7QHebow"
      },
      "source": [
        "#### 2. Distribution of categoric independent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLyv4ii9www4"
      },
      "source": [
        "For the independent categoric variables, we plot the bar plot to check the distribution of each variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7HE9bl9eboz"
      },
      "outputs": [],
      "source": [
        "# create a list of all categorical variables\n",
        "# initiate an empty list to store the categorical variables\n",
        "categorical=[]\n",
        "\n",
        "# use for loop to check the data type of each variable\n",
        "for column in df_features:\n",
        "    \n",
        "    # use 'if' statement with condition to check the categorical type \n",
        "    if is_string_dtype(df_features[column]):\n",
        "        \n",
        "        # append the variables with 'categoric' data type in the list 'categorical'\n",
        "        categorical.append(column)\n",
        "\n",
        "# plot the count plot for each categorical variable \n",
        "# set the number of rows in the subplot using the parameter, 'nrows'\n",
        "# set the number of columns in the subplot using the parameter, 'ncols'\n",
        "# 'figsize' sets the figure size\n",
        "fig, ax = plt.subplots(nrows = 3, ncols = 3, figsize=(25, 20))\n",
        "\n",
        "# use for loop to plot the count plot for each variable\n",
        "for variable, subplot in zip(categorical, ax.flatten()):\n",
        "    \n",
        "    # use countplot() to plot the graph\n",
        "    # pass the axes for the plot to the parameter, 'ax'\n",
        "    sns.countplot(df_Heart[variable], ax = subplot)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "row0AGsuwww6"
      },
      "source": [
        "#### 3. Distribution of dependent variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XAqBmkawww6"
      },
      "source": [
        "In section 4.1.1, we have split the dependent variable (target) and created a dataframe 'df_target'. Use this dataframe to check the distribution of target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M7lMgaWpwww6"
      },
      "outputs": [],
      "source": [
        "# get counts of 0's and 1's in the 'target' variable using 'value_counts()'\n",
        "# store the values in 'class_frequency'\n",
        "class_frequency = df_target.target.value_counts()\n",
        "class_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "STDtaBmgwww7"
      },
      "outputs": [],
      "source": [
        "# plot the countplot of the variable 'target'\n",
        "sns.countplot(x = df_target.target)\n",
        "\n",
        "# use below code to print the values in the graph\n",
        "# 'x' and 'y' gives position of the text\n",
        "# 's' is the text on the plot\n",
        "plt.text(x = -0.05, y = df_target.target.value_counts()[0] + 30, s = str((class_frequency[0])*100/len(df_target.target)) + '%')\n",
        "plt.text(x = 0.95, y = df_target.target.value_counts()[1] +20, s = str((class_frequency[1])*100/len(df_target.target)) + '%')\n",
        "\n",
        "# add plot and axes labels\n",
        "# set text size using 'fontsize'\n",
        "plt.title('Count Plot for Target Variable', fontsize = 15)\n",
        "plt.xlabel('Target Variable', fontsize = 15)\n",
        "plt.ylabel('Count', fontsize = 15)\n",
        "\n",
        "# to show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkZahcS9eboQ"
      },
      "source": [
        "<a id='correlation'></a>\n",
        "### 4.1.4 Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_oo8YEAeboS"
      },
      "source": [
        "Correlation is a statistic that measures the degree to which two variables move with each other. A correlation coefficient near  1  indicates the strong relationship between them; a weak correlation indicates the extent to which one variable increases as the other decreases. Correlation among multiple variables can be represented in the form of a matrix. This allows us to see which variables are correlated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4b-vSDZwww8"
      },
      "source": [
        "**1. Compute a correlation matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eqWwTr8eboZ"
      },
      "outputs": [],
      "source": [
        "# use the corr() function to generate the correlation matrix of the numeric variables\n",
        "corr = df_features.corr()\n",
        "\n",
        "# print the correlation matrix\n",
        "corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB3gxvJewww8"
      },
      "source": [
        "**2. Plot the heatmap for the diagonal correlation matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ShmM0sRwww9"
      },
      "source": [
        "A correlation matrix is a symmetric matrix. Plot only the lower triangular entries using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeXcIgFyebob",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# use 'mask' to plot a lower triangular correlation matrix \n",
        "# 'triu_indices_from' returns the indices for the upper-triangle of matrix\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask, k=1)] = True\n",
        "\n",
        "# plot the heat map\n",
        "# corr: give the correlation matrix\n",
        "# cmap: color code used for plotting\n",
        "# vmax: gives a maximum range of values for the chart\n",
        "# vmin: gives a minimum range of values for the chart\n",
        "# annot: prints the correlation values in the chart\n",
        "# annot_kws: sets the font size of the annotation\n",
        "# mask: mask the upper traingular matrix values\n",
        "sns.heatmap(corr, cmap = 'YlGnBu', vmax = 1.0, vmin = -1.0, annot = True, annot_kws = {\"size\": 12}, mask = mask)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjtDatMReboh"
      },
      "source": [
        "<a id='outliers'></a>\n",
        "### 4.1.5 Discover Outliers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv_JBRXxeboi"
      },
      "source": [
        "#### Importance of detecting an outlier\n",
        "An outlier is an observation that appears to deviate distinctly from other observations in the data. If the outliers are not removed, the model accuracy may decrease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miAEjo_hwww-"
      },
      "source": [
        "**1. Plot the boxplot for numeric data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGaqwSezeboj"
      },
      "outputs": [],
      "source": [
        "# plot a boxplot to visualize the outliers in all the numeric variables\n",
        "df_features.boxplot()\n",
        "\n",
        "# set plot label\n",
        "# set text size using 'fontsize'\n",
        "plt.title('Distribution of all Numeric Variables', fontsize = 15)\n",
        "\n",
        "# xticks() returns the x-axis ticks\n",
        "# 'rotation = vertical' rotates the x-axis labels vertically\n",
        "plt.xticks(rotation = 'vertical', fontsize = 15)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--zciCKbwww_"
      },
      "source": [
        "**Notice that the variables 'age' , 'old_peak' has a quite small range as compared to the other variables. Thus, it is difficult to see the outliers for these variables. So, we plot the boxplot only for the variables 'age', 'old_peak'.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA4Tsm6Hwww_"
      },
      "outputs": [],
      "source": [
        "# make a list of numerical features \n",
        "cols = [ 'age','old_peak']\n",
        "\n",
        "# plot multiple boxplots\n",
        "df_features[cols].boxplot()\n",
        "\n",
        "# set plot label\n",
        "# set text size using 'fontsize'\n",
        "plt.title('Distribution of Independent Variables age and old_peak', fontsize = 15)\n",
        "\n",
        "# xticks() returns the x-axis ticks\n",
        "# 'rotation = vertical' rotates the x-axis labels vertically\n",
        "plt.xticks(rotation = 'vertical', fontsize = 15)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz3aKA6Rwww_"
      },
      "source": [
        "**2. Note the variables for which outliers are present**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3etMxGumwwxA"
      },
      "source": [
        "**3. Remove outliers by IQR method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBc-FXUpebom"
      },
      "outputs": [],
      "source": [
        "# calculate the first quartile\n",
        "Q1 = df_features.quantile(0.25)\n",
        "\n",
        "# calculate the third quartile\n",
        "Q3 = df_features.quantile(0.75)\n",
        "\n",
        "# Interquartile Range (IQR) is defined as the difference between the third and first quartile\n",
        "# calculate IQR\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# print the IQR\n",
        "print(IQR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OGb5EZBpebop"
      },
      "outputs": [],
      "source": [
        "# remove the outliers from the dataframe 'df_Heart'\n",
        "# retrieve the dataframe without the outliers\n",
        "# '~' returns the values that do not satisfy the given conditions \n",
        "# i.e. it returns values between the range [Q1-1.5*IQR, Q3+1.5*IQR]\n",
        "# '|' is used as 'OR' operator on multiple conditions   \n",
        "# 'any(axis=1)' checks the entire row for atleast one 'True' entry (those rows represents outliers in the data)\n",
        "df_Heart = df_Heart[~((df_Heart < (Q1 - 1.5 * IQR)) | (df_Heart > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# reset the index of the dataframe without outliers\n",
        "df_Heart = df_Heart.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iuSrkGhebor"
      },
      "source": [
        "To confirm that the outliers have been removed; let us visualize the boxplot again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyt7WYCpwwxB"
      },
      "source": [
        "**4. Plot the boxplot to recheck for outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4dRHGd1wwxB"
      },
      "source": [
        "We plot the boxplots for all variables except for the variable `white corpuscle` for better visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZmatCI-ebos"
      },
      "outputs": [],
      "source": [
        "# make a list of numerical features without considering the 'white corpuscle'\n",
        "cols = ['age','rest_bps','cholestrol','thalach','old_peak']\n",
        "\n",
        "# plot multiple boxplots\n",
        "df_Heart[cols].boxplot()\n",
        "\n",
        "# set plot label\n",
        "# set text size using 'fontsize'\n",
        "plt.title('Distribution of Independent Variables', fontsize = 15)\n",
        "\n",
        "# xticks() returns the x-axis ticks\n",
        "# 'rotation = vertical' rotates the x-axis labels vertically\n",
        "plt.xticks(rotation = 'vertical', fontsize = 15)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_koYPUawwxB"
      },
      "source": [
        "Observing the range of the boxplot, we say that the outliers are removed from the original data.\n",
        "\n",
        "It is up to the discretion of the data scientist, to remove them or not; and maybe decide after evaluating the model performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAAk9vaIwwxB"
      },
      "source": [
        "A crude way to know whether the outliers have been removed or not is to check the dimensions of the data. If the dimensions are reduced that implies outliers are removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERHiDR7tebov"
      },
      "outputs": [],
      "source": [
        "# check the shape of the data after removal of outliers \n",
        "df_Heart.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-aTB0DseboJ"
      },
      "source": [
        "<a id='Missing_Values'></a>\n",
        "### 4.1.6 Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsY-ydhgeboM"
      },
      "outputs": [],
      "source": [
        "# sort the variables on the basis of total null values in the variable\n",
        "# 'isnull().sum()' returns the number of missing values in each variable\n",
        "# 'ascending = False' sorts values in the descending order\n",
        "# the variable with highest number of missing values will appear first\n",
        "Total = df_Heart.isnull().sum().sort_values(ascending = False)          \n",
        "\n",
        "# calculate the percentage of missing values\n",
        "# 'ascending = False' sorts values in the descending order\n",
        "# the variable with highest percentage of missing values will appear first\n",
        "Percent = (df_Heart.isnull().sum()*100/df_Heart.isnull().count()).sort_values(ascending = False)   \n",
        "\n",
        "# concat the 'Total' and 'Percent' columns using 'concat' function\n",
        "# 'keys' is the list of column names\n",
        "# 'axis = 1' concats along the columns\n",
        "missing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    \n",
        "missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zh6xtCUwwxD"
      },
      "source": [
        "Another way to find the missing values is to plot a heatmap for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or14vR2EwwxD"
      },
      "outputs": [],
      "source": [
        "# plot heatmap to check null values\n",
        "# 'cbar = False' does not show the color axis \n",
        "sns.heatmap(df_Heart.isnull(), cbar=False)\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hvE0dwNwwxD"
      },
      "source": [
        "There are no horizontal lines in the heatmap which would correspond to a probable missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8EfOhNXwwxE"
      },
      "source": [
        "<a id='Data_Preparation'></a>\n",
        "## 4.2 Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcD70E9wwxE"
      },
      "source": [
        "To build the classification models, we need to encode the categorical variables using dummy encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA5hnr6NwwxF"
      },
      "source": [
        "**1. Filter numerical and categorical variables **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DLtmzY72eboU"
      },
      "outputs": [],
      "source": [
        "# dataframe with categorical features\n",
        "# 'categorical' contains a list of categorical variables\n",
        "df_cat = df_Heart[categorical]\n",
        "\n",
        "# dataframe with numerical features\n",
        "# use 'drop()' to drop the categorical variables\n",
        "# 'axis = 1' drops the corresponding column(s)\n",
        "df_num = df_Heart.drop(categorical, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgrynyk-wwxF"
      },
      "source": [
        "**2. Dummy encode the categorical variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gzj44_nDwwxF"
      },
      "outputs": [],
      "source": [
        "# print the first five observations of the 'df_cat'\n",
        "df_cat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rLNKCK7lebo3"
      },
      "outputs": [],
      "source": [
        "# use 'get_dummies()' from pandas to create dummy variables\n",
        "# use 'drop_first = True' to create (n-1) dummy variables\n",
        "df_cat_dummies = pd.get_dummies(df_cat, drop_first = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYPnUX59wwxG"
      },
      "outputs": [],
      "source": [
        "# check the first five observations of the data with dummy encoded variables\n",
        "df_cat_dummies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xmta2gcwwxG"
      },
      "source": [
        "**3. Concatenate numerical and dummy encoded categorical variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U90M-Hokebo7"
      },
      "outputs": [],
      "source": [
        "# concat the dummy variables with numeric features to create a dataframe of all independent variables\n",
        "# 'axis=1' concats the dataframes along columns \n",
        "df_Heart_dummy = pd.concat([df_num, df_cat_dummies], axis=1)\n",
        "\n",
        "# display first five observations of the dummy dataframe\n",
        "df_Heart_dummy.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrfTrS4BwwxG"
      },
      "source": [
        "After removal of outliers and missing values in the data, the dataframe `df_Heart_dummy` contains independent as well as dependent variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IBKWuemowwxH"
      },
      "outputs": [],
      "source": [
        "# select only the target variable 'target' and store it in dataframe 'y'\n",
        "y = pd.DataFrame(df_Heart_dummy['target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlmnRIfAwwxH"
      },
      "source": [
        "Now, use this 'y' as a target variable to build the classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b5jHXkbdwwxH"
      },
      "outputs": [],
      "source": [
        "# use 'drop()' to remove the variable 'target' from df_target_dummy\n",
        "# 'axis = 1' drops the corresponding column(s)\n",
        "X = df_Heart_dummy.drop('target',axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQstRNsZwwxH"
      },
      "outputs": [],
      "source": [
        "# check the first five observations of X\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ0p99GwwwxI"
      },
      "source": [
        "Use this 'X' as a set of predictors to build the classification models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAUZmXiwwwxI"
      },
      "source": [
        "#### Create a generalized function to calculate the metrics for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cefb3fdcwwxI"
      },
      "outputs": [],
      "source": [
        "# create a generalized function to calculate the metrics values for test set\n",
        "def get_test_report(model):\n",
        "    \n",
        "    # return the performace measures on test set\n",
        "    return(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjRIIkWnwwxI"
      },
      "source": [
        "#### Create a generalized function to calculate the kappa score for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YcuWTju0wwxJ"
      },
      "outputs": [],
      "source": [
        "# create a generalized function to calculate the metrics values for test set\n",
        "def kappa_score(model):\n",
        "    \n",
        "    # return the kappa score on test set\n",
        "    return(cohen_kappa_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly5aN_X8wwxJ"
      },
      "source": [
        "#### Define a function to plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Av39nL7SebpS"
      },
      "outputs": [],
      "source": [
        "# define a to plot a confusion matrix for the model\n",
        "def plot_confusion_matrix(model):\n",
        "    \n",
        "    # create a confusion matrix\n",
        "    # pass the actual and predicted target values to the confusion_matrix()\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # label the confusion matrix  \n",
        "    # pass the matrix as 'data'\n",
        "    # pass the required column names to the parameter, 'columns'\n",
        "    # pass the required row names to the parameter, 'index'\n",
        "    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n",
        "\n",
        "    # plot a heatmap to visualize the confusion matrix\n",
        "    # 'annot' prints the value of each grid \n",
        "    # 'fmt = d' returns the integer value in each grid\n",
        "    # 'cmap' assigns color to each grid\n",
        "    # as we do not require different colors for each grid in the heatmap,\n",
        "    # use 'ListedColormap' to assign the specified color to the grid\n",
        "    # 'cbar = False' will not return the color bar to the right side of the heatmap\n",
        "    # 'linewidths' assigns the width to the line that divides each grid\n",
        "    # 'annot_kws = {'size':25})' assigns the font size of the annotated text \n",
        "    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n",
        "                linewidths = 0.1, annot_kws = {'size':25})\n",
        "\n",
        "    # set the font size of x-axis ticks using 'fontsize'\n",
        "    plt.xticks(fontsize = 20)\n",
        "\n",
        "    # set the font size of y-axis ticks using 'fontsize'\n",
        "    plt.yticks(fontsize = 20)\n",
        "\n",
        "    # display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7id6ZNGawwxK"
      },
      "source": [
        "#### Define a function to plot the ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FTt7Jkd6wwxK"
      },
      "outputs": [],
      "source": [
        "# define a function to plot the ROC curve and print the ROC-AUC score\n",
        "def plot_roc(model):\n",
        "    \n",
        "    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n",
        "    # pass the actual target values and predicted probabilities to the function\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "    # plot the ROC curve\n",
        "    plt.plot(fpr, tpr)\n",
        "\n",
        "    # set limits for x and y axes\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "\n",
        "    # plot the straight line showing worst prediction for the model\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "\n",
        "    # add plot and axes labels\n",
        "    # set text size using 'fontsize'\n",
        "    plt.title('ROC Curve for Heart Disease Classifier', fontsize = 15)\n",
        "    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n",
        "    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n",
        "\n",
        "    # add the AUC score to the plot\n",
        "    # 'x' and 'y' gives position of the text\n",
        "    # 's' is the text \n",
        "    # use round() to round-off the AUC score upto 4 digits\n",
        "    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n",
        "\n",
        "    # plot the grid\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwNiU2k-wwxK"
      },
      "source": [
        "#### Create a generalized function to create a dataframe containing the scores for the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OpKZly82wwxL"
      },
      "outputs": [],
      "source": [
        "# create an empty dataframe to store the scores for various classification algorithms\n",
        "score_card = pd.DataFrame(columns=['Model', 'AUC Score', 'Precision Score', 'Recall Score', 'Accuracy Score',\n",
        "                                   'Kappa Score', 'f1-score'])\n",
        "\n",
        "# append the result table for all performance scores\n",
        "# performance measures considered for comparision are 'AUC', 'Precision', 'Recall','Accuracy','Kappa Score', and 'f1-score'\n",
        "# compile the required information in a user defined function \n",
        "def update_score_card(model_name):\n",
        "    \n",
        "    # assign 'score_card' as global variable\n",
        "    global score_card\n",
        "\n",
        "    # append the results to the dataframe 'score_card'\n",
        "    # 'ignore_index = True' do not consider the index labels\n",
        "    score_card = score_card.append({'Model': model_name,\n",
        "                                    'AUC Score' : roc_auc_score(y_test, y_pred_prob),\n",
        "                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n",
        "                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n",
        "                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n",
        "                                    'Kappa Score': cohen_kappa_score(y_test, y_pred),\n",
        "                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n",
        "                                    ignore_index = True)\n",
        "    return(score_card)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvj6SJi9ebqY"
      },
      "source": [
        "<a id='DecisionTree'> </a>\n",
        "# 5. Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnPBtZ5jwwxL"
      },
      "source": [
        "Decision Tree is a non-parametric supervised learning method. It builds a regression model in the form of a tree structure. It breaks down a data set into smaller and smaller subsets, which is called splitting. The final result is a tree with a decision and leaf nodes. A decision node has two or more branches. The leaf node represents a class or decision. The topmost decision node in a tree that corresponds to the best predictor called 'root node'. The decision tree is built using different criteria like gini index, and entropy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZRHUDo3wwxM"
      },
      "source": [
        "<a id='DecisionTreeWFS'> </a>\n",
        "## 5.1 Decision Tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0q_T_eEwwxM"
      },
      "source": [
        "**1. Split the data into training and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Bw0doXwwxM"
      },
      "outputs": [],
      "source": [
        "# split data into train subset and test subset\n",
        "# set 'random_state' to generate the same dataset each time you run the code \n",
        "# 'test_size' returns the proportion of data to be included in the test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 10)\n",
        "\n",
        "# check the dimensions of the train & test subset using 'shape'\n",
        "# print dimension of train set\n",
        "print(\"X_train\",X_train.shape)\n",
        "print(\"y_train\",y_train.shape)\n",
        "\n",
        "# print dimension of test set\n",
        "print(\"X_test\",X_test.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg6xztN7wwxN"
      },
      "source": [
        "**2. Build the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRgAI_4uwwxN"
      },
      "source": [
        "We build the decision tree on the `unscaled features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r5SNCxwgebqY"
      },
      "outputs": [],
      "source": [
        "# instantiate the 'DecisionTreeClassifier' object using 'entropy' criterion\n",
        "# pass the 'random_state' to obtain the same samples for each time you run the code\n",
        "decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\n",
        "\n",
        "# fit the model using fit() on train data\n",
        "decision_tree_model = decision_tree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlZkMH_ewwxN"
      },
      "source": [
        "#### 3. Plot the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhSS6djUwwxO"
      },
      "outputs": [],
      "source": [
        "# save the column names in 'labels'\n",
        "lables = X_train.columns\n",
        "\n",
        "# plot the decision tree \n",
        "fig = plt.figure(figsize=(30,30))\n",
        "_ = tree.plot_tree(decision_tree_model, \n",
        "                   feature_names=lables,  \n",
        "                   class_names=[\"0\",\"1\"],\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXhn4AOHwwxO"
      },
      "outputs": [],
      "source": [
        "# DOT data\n",
        "dot_data = tree.export_graphviz(decision_tree_model, out_file=None, \n",
        "                                feature_names=lables,  \n",
        "                                class_names=[\"0\",\"1\"],\n",
        "                                filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLAT7fyvwwxO"
      },
      "source": [
        "**4. Do predictions on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nPwP2i13wwxO"
      },
      "outputs": [],
      "source": [
        "# predict probabilities on the test set\n",
        "# consider the probability of positive class by subsetting with '[:,1]'\n",
        "y_pred_prob = decision_tree_model.predict_proba(X_test)[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uvQXQNGlwwxP"
      },
      "outputs": [],
      "source": [
        "# predict the class labels using 'X_test'\n",
        "y_pred = decision_tree_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-X27qPwwxP"
      },
      "source": [
        "**5. Compute accuracy measures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWNel1IwwxP"
      },
      "source": [
        "#### Build a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afq7bBc8wwxP"
      },
      "outputs": [],
      "source": [
        "# call the function to plot the confusion matrix\n",
        "# pass the decision tree model to the function\n",
        "plot_confusion_matrix(decision_tree_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqUhFKCJwwxP"
      },
      "source": [
        "**Calculate performance measures on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP5DA9_qebpC",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# compute the performance measures on test data\n",
        "# call the function 'get_test_report'\n",
        "# pass the decision tree model to the function\n",
        "test_report = get_test_report(decision_tree_model)\n",
        "\n",
        "# print the performace measures\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2GNJgOfwwxQ"
      },
      "source": [
        "**Interpretation:** The accuracy is 71% for this model. Also, the sensitivity and specificity of the model is quite different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLK9yZeMwwxQ"
      },
      "outputs": [],
      "source": [
        "# compute kappa score on test set\n",
        "# call the function 'kappa_score'\n",
        "# pass the decision tree model to the function\n",
        "kappa_value = kappa_score(decision_tree_model)\n",
        "\n",
        "# print the kappa value\n",
        "print(kappa_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q7Sh-wbwwxQ"
      },
      "source": [
        "**Interpretation:** As the kappa score for the decision tree (pruned) is 0.4198, we can say that there is moderate agreement between the actual and predicted values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHRWRWuHwwxQ"
      },
      "source": [
        "**Plot the ROC curve.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Xsti84LowwxQ"
      },
      "outputs": [],
      "source": [
        "# call the function 'plot_roc' to plot the ROC curve\n",
        "# pass the decision tree model to the function\n",
        "plot_roc(decision_tree_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDWRop8PwwxR"
      },
      "source": [
        "**7. Tabulate the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYjpYGbfwwxR"
      },
      "outputs": [],
      "source": [
        "# use the function 'update_score_card' to store the performance measures\n",
        "# pass the 'Decision Tree' as model name to the function\n",
        "update_score_card(model_name = 'Decision Tree')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnnMZei4wwxR"
      },
      "source": [
        "<a id='DecisionTreePruning'></a>\n",
        "## 5.2 Prune a Decision Tree "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP6RFDOOwwxS"
      },
      "source": [
        "**1. Prune the decision tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMKyTF8LwwxS"
      },
      "source": [
        "We prune the decision tree by specifying the maximum depth and maximum number of leaves of the tree. \n",
        "\n",
        "We use the unscaled features to build the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IMjieFzEwwxS"
      },
      "outputs": [],
      "source": [
        "# instantiate the 'DecisionTreeClassifier' object\n",
        "# max_depth: maximum depth of the tree \n",
        "# max_leaf_nodes: maximum number of leaf nodes in the tree\n",
        "# pass the 'random_state' to obtain the same samples for each time you run the code\n",
        "prune = DecisionTreeClassifier(max_depth = 5, max_leaf_nodes = 25 , random_state = 10)\n",
        "\n",
        "# fit the model using fit() on train data\n",
        "decision_tree_prune = prune.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TcgxfNewwxS"
      },
      "source": [
        "#### 2. Plot the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Fp7sv8FuwwxT"
      },
      "outputs": [],
      "source": [
        "# save the column names in 'labels'\n",
        "lables = X_train.columns\n",
        "\n",
        "# plot the decision tree \n",
        "fig = plt.figure(figsize=(30,30))\n",
        "_ = tree.plot_tree(decision_tree_prune, \n",
        "                   feature_names=lables,  \n",
        "                   class_names=[\"0\",\"1\"],\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqW13lI4wwxT"
      },
      "outputs": [],
      "source": [
        "# DOT data\n",
        "dot_data = tree.export_graphviz(decision_tree_prune, out_file=None, \n",
        "                                feature_names=lables,  \n",
        "                                class_names=[\"0\",\"1\"],\n",
        "                                filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X-6ZrhawwxT"
      },
      "source": [
        "**3. Do predictions on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GenDumH3wwxU"
      },
      "outputs": [],
      "source": [
        "# predict probabilities on the test set\n",
        "# consider the probability of positive class by subsetting with '[:,1]'\n",
        "y_pred_prob = decision_tree_prune.predict_proba(X_test)[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "snVfD_AywwxU"
      },
      "outputs": [],
      "source": [
        "# predict the class labels using 'X_test'\n",
        "y_pred = decision_tree_prune.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A3MH6MDwwxU"
      },
      "source": [
        "**4. Compute accuracy measures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLuJWQA6wwxV"
      },
      "source": [
        "#### Build a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnGRsR1twwxV"
      },
      "outputs": [],
      "source": [
        "# call the function to plot the confusion matrix\n",
        "# pass the decision tree (pruned) model to the function\n",
        "plot_confusion_matrix(decision_tree_prune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXCt-ev-wwxV"
      },
      "source": [
        "**Calculate performance measures on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "zGRTmMdpwwxV"
      },
      "outputs": [],
      "source": [
        "# compute the performance measures on test data\n",
        "# call the function 'get_test_report'\n",
        "# pass the decision tree (pruned) model to the function\n",
        "test_report = get_test_report(decision_tree_prune)\n",
        "\n",
        "# print the performace measures\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLdP9gitwwxW"
      },
      "source": [
        "**Interpretation:** The accuracy is 72% for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNuPTbvowwxW"
      },
      "outputs": [],
      "source": [
        "# compute kappa score on test set\n",
        "# call the function 'kappa_score'\n",
        "# pass the decision tree (pruned) model to the function\n",
        "kappa_value = kappa_score(decision_tree_prune)\n",
        "\n",
        "# print the kappa value\n",
        "print(kappa_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVI2wW7wwxW"
      },
      "source": [
        "**Interpretation:** As the kappa score for the decision tree (pruned) is 0.4421, we can say that there is moderate agreement between the actual and predicted values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xMF2834wwxX"
      },
      "source": [
        "**Plot the ROC curve.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "6cSJmAQewwxX"
      },
      "outputs": [],
      "source": [
        "# call the function 'plot_roc' to plot the ROC curve\n",
        "# pass the decision tree (pruned) model to the function\n",
        "plot_roc(decision_tree_prune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQgHT2OtwwxY"
      },
      "source": [
        "**5. Tabulate the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jQ63P53AwwxY"
      },
      "outputs": [],
      "source": [
        "# use the function 'update_score_card' to store the performance measures\n",
        "# pass the 'Decision Tree (Pruned)' as model name to the function\n",
        "update_score_card(model_name = 'Decision Tree (Pruned)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbs7RQIbebqn"
      },
      "source": [
        "<a id='DecisionTreewithGridSearchCv'> </a>\n",
        "## 5.3 Decision Tree (using GridSearchCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAI4j9eHwwxY"
      },
      "source": [
        "Now we show how a decision tree is optimized by cross-validation, which is done using the `GridSearchCV()` from sklearn library.\n",
        "\n",
        "The performance of the selected hyperparameters and trained model is then measured on the test set that was not used during the model building."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuVwlH9qwwxZ"
      },
      "source": [
        "**1. Use GridSearch to obtain the optimal values of hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N5olYIn_wwxZ"
      },
      "outputs": [],
      "source": [
        "# create a dictionary with hyperparameters and its values\n",
        "# pass the criteria 'entropy' and 'gini' to the parameter, 'criterion' \n",
        "# pass the list of values to 'min_samples_split' that assigns minimum number of samples to split an internal node\n",
        "# pass the list of values to 'max_depth' that assigns maximum depth of the tree\n",
        "# pass the list of values to 'min_samples_leaf' that assigns minimum number of samples required at the terminal/leaf node\n",
        "# pass the list of values to 'max_leaf_nodes' that assigns maximum number of leaf nodes in the tree\n",
        "tuned_paramaters = [{'criterion': ['gini', 'entropy'],\n",
        "                     'min_samples_split': [10, 20, 30],\n",
        "                     'max_depth': [3, 5, 7, 9],\n",
        "                     'min_samples_leaf': [15, 20, 25, 30, 35],\n",
        "                     'max_leaf_nodes': [5, 10, 15, 20, 25]}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvMv41LOwwxa"
      },
      "outputs": [],
      "source": [
        "# instantiate the 'DecisionTreeClassifier' \n",
        "# pass the 'random_state' to obtain the same samples for each time you run the code\n",
        "decision_tree_classification = DecisionTreeClassifier(random_state = 10)\n",
        "\n",
        "# use GridSearchCV() to find the optimal value of the hyperparameters\n",
        "# estimator: pass the decision tree classifier model\n",
        "# param_grid: pass the list 'tuned_parameters'\n",
        "# cv: number of folds in k-fold i.e. here cv = 10\n",
        "grid = GridSearchCV(estimator = decision_tree_classification, \n",
        "                         param_grid = tuned_paramaters, \n",
        "                         cv = 10)\n",
        "\n",
        "# fit the model on X_train and y_train using fit()\n",
        "dt_grid = grid.fit(X_train, y_train)\n",
        "\n",
        "# get the best parameters\n",
        "print('Best parameters for decision tree classifier: ', dt_grid.best_params_, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4QpavaJwwxa"
      },
      "source": [
        "**2. Build the model using the hyperparameters obtained in step 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hn5WcBvlwwxa"
      },
      "outputs": [],
      "source": [
        "# instantiate the 'DecisionTreeClassifier'\n",
        "# 'best_params_' returns the dictionary containing best parameter values and parameter name  \n",
        "# 'get()' returns the value of specified parameter\n",
        "# pass the 'random_state' to obtain the same samples for each time you run the code\n",
        "dt_grid_model = DecisionTreeClassifier(criterion = dt_grid.best_params_.get('criterion'),\n",
        "                                       max_depth = dt_grid.best_params_.get('max_depth'),\n",
        "                                       max_leaf_nodes = dt_grid.best_params_.get('max_leaf_nodes'),\n",
        "                                       min_samples_leaf = dt_grid.best_params_.get('min_samples_leaf'),\n",
        "                                       min_samples_split = dt_grid.best_params_.get('min_samples_split'),\n",
        "                                       random_state = 10)\n",
        "\n",
        "# use fit() to fit the model on the train set\n",
        "dt_grid_model = dt_grid_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ly_LJ1wwxb"
      },
      "source": [
        "#### 3. Plot the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6En1lXC9wwxb"
      },
      "outputs": [],
      "source": [
        "# save the column names in 'labels'\n",
        "lables = X_train.columns\n",
        "\n",
        "# plot the decision tree \n",
        "fig = plt.figure(figsize=(30,30))\n",
        "_ = tree.plot_tree(dt_grid_model, \n",
        "                   feature_names=lables,  \n",
        "                   class_names=[\"0\",\"1\"],\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpDGWdTawwxb"
      },
      "outputs": [],
      "source": [
        "# DOT data\n",
        "dot_data = tree.export_graphviz(dt_grid_model, out_file=None, \n",
        "                                feature_names=lables,  \n",
        "                                class_names=[\"0\",\"1\"],\n",
        "                                filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7zDaR-Nwwxc"
      },
      "source": [
        "**4. Do predictions on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Y22yzxxwwxc"
      },
      "outputs": [],
      "source": [
        "# predict probabilities on the test set\n",
        "# consider the probability of positive class by subsetting with '[:,1]'\n",
        "y_pred_prob = dt_grid_model.predict_proba(X_test)[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cORd3zVlwwxc"
      },
      "outputs": [],
      "source": [
        "# predict the class labels using 'X_test'\n",
        "y_pred = dt_grid_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7nkQzT1wwxd"
      },
      "source": [
        "**5. Compute accuracy measures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ploNjcuTwwxd"
      },
      "source": [
        "#### Build a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n_LA6SNwwxd"
      },
      "outputs": [],
      "source": [
        "# call the function to plot the confusion matrix\n",
        "# pass the decision tree (GridSearchCV) model to the function\n",
        "plot_confusion_matrix(dt_grid_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CFJ_3fuwwxd"
      },
      "source": [
        "**Calculate performance measures on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "_vxBjymnwwxd"
      },
      "outputs": [],
      "source": [
        "# compute the performance measures on test data\n",
        "# call the function 'get_test_report'\n",
        "# pass the decision tree (GridSearchCV) model to the function\n",
        "test_report = get_test_report(dt_grid_model)\n",
        "\n",
        "# print the performace measures\n",
        "print(test_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0-BFzXkwwxe"
      },
      "source": [
        "**Interpretation:** The accuracy is 72% for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVbtrtxXwwxe"
      },
      "outputs": [],
      "source": [
        "# compute kappa score on test set\n",
        "# call the function 'kappa_score'\n",
        "# pass the decision tree (GridSearchCV) model to the function\n",
        "kappa_value = kappa_score(dt_grid_model)\n",
        "\n",
        "# print the kappa value\n",
        "print(kappa_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2GfXgTTwwxe"
      },
      "source": [
        "**Interpretation:** As the kappa score for the decision tree (GridSearchCV) is 0.4379, we can say that there is moderate agreement between the actual and predicted values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYz6XWbnwwxe"
      },
      "source": [
        "**Plot the ROC curve.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "JLQhG_L4wwxe"
      },
      "outputs": [],
      "source": [
        "# call the function 'plot_roc' to plot the ROC curve\n",
        "# pass the decision tree (GridSearchCV) model to the function\n",
        "plot_roc(dt_grid_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f075O_cJwwxf"
      },
      "source": [
        "<a id=\"conclusion\"> </a>\n",
        "# 6. Conclusion and Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCGtenJiwwxi"
      },
      "source": [
        "To take the final conclusion, let us print the result table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTWGMSDhwwxj"
      },
      "source": [
        "**Tabulate the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2udmW7hPwwxj"
      },
      "outputs": [],
      "source": [
        "# use the function 'update_score_card' to store the performance measures\n",
        "# pass the 'Decision Tree (GridSearchCV)' as model name to the function\n",
        "update_score_card(model_name = 'Decision Tree (GridSearchCV)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cwK6Hb4Qwwxk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Heart Disease Prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}